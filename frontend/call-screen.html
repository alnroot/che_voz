<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>En llamada</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      background: linear-gradient(135deg, #1e1e1e 0%, #2d2d2d 100%);
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
      color: white;
      height: 100vh;
      display: flex;
      flex-direction: column;
      overflow: hidden;
    }

    .status-bar {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 8px 20px;
      font-size: 14px;
      background: rgba(0, 0, 0, 0.3);
    }

    .status-bar .time {
      font-weight: 600;
    }

    .status-bar .icons {
      display: flex;
      gap: 5px;
    }

    .call-info {
      flex: 1;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      padding: 20px;
    }

    .avatar {
      width: 120px;
      height: 120px;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 48px;
      margin-bottom: 20px;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
    }

    .contact-name {
      font-size: 32px;
      font-weight: 300;
      margin-bottom: 8px;
    }

    .call-status {
      font-size: 16px;
      color: #aaa;
      margin-bottom: 8px;
      cursor: pointer;
      padding: 10px 20px;
      border-radius: 20px;
      transition: all 0.3s ease;
    }
    
    .call-status.clickable {
      background: rgba(76, 175, 80, 0.2);
      border: 1px solid #4CAF50;
      color: #4CAF50;
    }
    
    .call-status.clickable:hover {
      background: rgba(76, 175, 80, 0.3);
    }

    .timer {
      font-size: 18px;
      color: #4CAF50;
      font-weight: 500;
    }

    .call-actions {
      background: rgba(0, 0, 0, 0.5);
      backdrop-filter: blur(20px);
      padding: 30px 20px 40px;
      border-radius: 30px 30px 0 0;
    }

    .action-grid {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 20px;
      max-width: 400px;
      margin: 0 auto;
    }

    .action-btn {
      width: 65px;
      height: 65px;
      border-radius: 50%;
      border: none;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      cursor: pointer;
      transition: all 0.3s ease;
      background: rgba(255, 255, 255, 0.1);
      color: white;
      position: relative;
      margin: 0 auto;
    }

    .action-btn:active {
      transform: scale(0.95);
    }

    .action-btn.active {
      background: rgba(255, 255, 255, 0.3);
    }

    .action-btn svg {
      width: 24px;
      height: 24px;
      fill: currentColor;
    }

    .action-btn span {
      position: absolute;
      bottom: -20px;
      font-size: 12px;
      color: #ccc;
      white-space: nowrap;
    }

    .end-call {
      background: #e53935;
      grid-column: 2;
    }

    .end-call:hover {
      background: #c62828;
    }

    .add-call {
      opacity: 0.5;
    }
  </style>
  <script src="/static/audio-converter.js"></script>
</head>
<body>

  <div class="status-bar">
    <div class="carrier">Carrier</div>
    <div class="time" id="currentTime">9:41</div>
    <div class="icons">
      <span>ðŸ“¶</span>
      <span>ðŸ”‹</span>
    </div>
  </div>

  <div class="call-info">
    <div class="avatar">ðŸ‘¤</div>
    <div class="contact-name">Che Voz</div>
    <div class="call-status">mÃ³vil</div>
    <div class="timer" id="timer">00:00</div>
  </div>

  <div class="call-actions">
    <div class="action-grid">
      <button class="action-btn" id="speakerBtn">
        <svg viewBox="0 0 24 24">
          <path d="M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z"/>
        </svg>
        <span>Altavoz</span>
      </button>
      
      <button class="action-btn" id="videoBtn">
        <svg viewBox="0 0 24 24">
          <path d="M17 10.5V7c0-.55-.45-1-1-1H4c-.55 0-1 .45-1 1v10c0 .55.45 1 1 1h12c.55 0 1-.45 1-1v-3.5l4 4v-11l-4 4z"/>
        </svg>
        <span>FaceTime</span>
      </button>
      
      <button class="action-btn" id="muteBtn">
        <svg viewBox="0 0 24 24">
          <path d="M12 14c1.66 0 2.99-1.34 2.99-3L15 5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm5.3-3c0 3-2.54 5.1-5.3 5.1S6.7 14 6.7 11H5c0 3.41 2.72 6.23 6 6.72V21h2v-3.28c3.28-.48 6-3.3 6-6.72h-1.7z"/>
        </svg>
        <span>Silenciar</span>
      </button>
      
      <button class="action-btn add-call">
        <svg viewBox="0 0 24 24">
          <path d="M19 13h-6v6h-2v-6H5v-2h6V5h2v6h6v2z"/>
        </svg>
        <span>AÃ±adir</span>
      </button>
      
      <button class="action-btn end-call" id="endBtn">
        <svg viewBox="0 0 24 24">
          <path d="M12 9c-1.6 0-3.15.25-4.6.72v3.1c0 .39-.23.74-.56.9-.98.49-1.87 1.12-2.66 1.85-.18.18-.43.28-.68.28-.3 0-.56-.13-.74-.33l-3.46-3.45c-.17-.18-.26-.43-.26-.7 0-.28.09-.52.26-.7C3.15 7.05 7.46 5 12 5s8.85 2.05 12.74 5.65c.17.18.26.43.26.7 0 .28-.09.52-.26.7l-3.46 3.45c-.18.2-.45.33-.74.33-.26 0-.51-.1-.69-.28-.79-.73-1.68-1.36-2.66-1.85-.33-.16-.56-.51-.56-.9v-3.1C15.15 9.25 13.6 9 12 9z"/>
        </svg>
        <span>Finalizar</span>
      </button>
      
      <button class="action-btn">
        <svg viewBox="0 0 24 24">
          <path d="M19 17h-2v2h2v-2zm0-4h-2v2h2v-2zm0-4h-2v2h2V9zm-4 8h-2v2h2v-2zm0-4h-2v2h2v-2zm0-4h-2v2h2V9zm0-4h-2v2h2V5zm-4 12h-2v2h2v-2zm0-4h-2v2h2v-2zm0-4H9v2h2V9zm0-4H9v2h2V5zM7 17H5v2h2v-2zm0-4H5v2h2v-2zm0-4H5v2h2V9z"/>
        </svg>
        <span>Teclado</span>
      </button>
    </div>
  </div>

  <script>
    let seconds = 0;
    const timerEl = document.getElementById("timer");
    const currentTimeEl = document.getElementById("currentTime");
    const callStatusEl = document.querySelector('.call-status');
    const contactNameEl = document.querySelector('.contact-name');
    
    // WebSocket and audio variables
    let ws = null;
    let conversationId = null;
    let mediaRecorder = null;
    let audioContext = null;
    let audioQueue = [];
    let isPlaying = false;
    let currentAudioSource = null;
    let timerInterval = null;
    let audioConverter = null;

    // Get phone number from URL params
    const urlParams = new URLSearchParams(window.location.search);
    const phoneNumber = urlParams.get('from') || urlParams.get('number') || '+54 11 1234-5678';

    function formatTime(sec) {
      const m = String(Math.floor(sec / 60)).padStart(2, '0');
      const s = String(sec % 60).padStart(2, '0');
      return `${m}:${s}`;
    }

    function updateCurrentTime() {
      const now = new Date();
      const hours = now.getHours();
      const minutes = String(now.getMinutes()).padStart(2, '0');
      currentTimeEl.textContent = `${hours}:${minutes}`;
    }

    async function initializeCall() {
      try {
        console.log('ðŸš€ Initializing call with number:', phoneNumber);
        callStatusEl.textContent = 'Llamando...';
        
        // Call the dial endpoint
        const response = await fetch('/call/dial', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ phone_number: phoneNumber })
        });

        if (!response.ok) throw new Error('Failed to initialize call');

        const data = await response.json();
        console.log('ðŸ“ž Call initialized:', data);
        conversationId = data.conversation_id;
        contactNameEl.textContent = data.agent_name;
        
        // Connect WebSocket
        connectWebSocket(data.websocket_url);
        
      } catch (error) {
        console.error('Error initializing call:', error);
        callStatusEl.textContent = 'Error de conexiÃ³n';
        setTimeout(() => window.location.href = 'dialer.html', 2000);
      }
    }

    function connectWebSocket(wsUrl) {
      // Use wss:// for production, ws:// for localhost
      const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
      const fullUrl = `${protocol}//${window.location.host}${wsUrl}`;
      ws = new WebSocket(fullUrl);

      ws.onopen = async () => {
        console.log('âœ… WebSocket connected');
        callStatusEl.textContent = 'Conectando...';
        
        // Add a click handler to start audio on user interaction
        const startAudioOnClick = async () => {
          if (!mediaRecorder) {
            console.log('ðŸŽ¤ Starting audio on user interaction');
            callStatusEl.classList.remove('clickable');
            callStatusEl.textContent = 'Conectando micrÃ³fono...';
            try {
              await initializeAudio();
              // Send ready message after audio initialization
              if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ type: 'ready' }));
              }
            } catch (e) {
              console.error('Failed to initialize audio:', e);
              callStatusEl.textContent = 'Error al acceder al micrÃ³fono';
            }
          }
        };
        
        // Make the status clickable immediately
        callStatusEl.textContent = 'ðŸŽ¤ Haz clic aquÃ­ para iniciar la llamada';
        callStatusEl.classList.add('clickable');
        callStatusEl.style.cursor = 'pointer';
        
        // Add click handler to the status element
        callStatusEl.addEventListener('click', startAudioOnClick);
        
        // Also add to document for any click
        document.addEventListener('click', startAudioOnClick, { once: true });
      };

      ws.onmessage = async (event) => {
        const data = JSON.parse(event.data);
        
        switch(data.type) {
          case 'ready':
            callStatusEl.textContent = 'En llamada';
            console.log('Call ready, starting timer');
            // Start timer
            timerInterval = setInterval(() => {
              seconds++;
              timerEl.textContent = formatTime(seconds);
            }, 1000);
            break;
          
          case 'conversation_started':
            console.log('Conversation active:', data.conversation_id);
            break;
          
          case 'audio':
            if (data.data) {
              console.log(`Received audio from backend, size: ${data.data.length}`);
              audioQueue.push(data.data);
              if (!isPlaying) playNextAudio();
            }
            break;
          
          case 'user_transcript':
            console.log('User said:', data.text);
            break;
          
          case 'agent_response':
            console.log('Agent said:', data.text);
            break;
          
          case 'error':
            console.error('Error:', data.message);
            callStatusEl.textContent = 'Error en la llamada';
            break;
        }
      };

      ws.onerror = (error) => {
        console.error('WebSocket error:', error);
        callStatusEl.textContent = 'Error de conexiÃ³n';
      };

      ws.onclose = () => {
        if (timerInterval) clearInterval(timerInterval);
        
        // Clean up audio nodes
        if (window.audioProcessor) {
          window.audioProcessor.disconnect();
          window.audioSource.disconnect();
          window.isRecording = false;
        }
      };
    }

    async function initializeAudio() {
      try {
        console.log('ðŸŽµ Initializing audio...');
        
        // Create AudioContext with user gesture handling
        if (!audioContext) {
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
          console.log('ðŸ“Š AudioContext created, state:', audioContext.state);
        }
        
        // Always try to resume the context
        if (audioContext.state === 'suspended') {
          console.log('AudioContext suspended, attempting to resume...');
          await audioContext.resume();
          console.log('AudioContext resumed successfully');
        }
        
        // Request microphone
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            sampleRate: 16000,
            channelCount: 1
          } 
        });
        
        // Use Web Audio API to capture raw PCM instead of MediaRecorder
        console.log('AudioContext sample rate:', audioContext.sampleRate);
        
        // If sample rate is not 16000, we need to resample
        const needsResampling = audioContext.sampleRate !== 16000;
        if (needsResampling) {
          console.log('Will resample from', audioContext.sampleRate, 'to 16000 Hz');
        }
        
        const source = audioContext.createMediaStreamSource(stream);
        const processor = audioContext.createScriptProcessor(4096, 1, 1);
        
        let audioChunksSent = 0;
        let isRecording = true;
        let isSpeaking = false;
        let silenceCounter = 0;
        let speechCounter = 0;
        
        processor.onaudioprocess = (e) => {
          if (!isRecording || !ws || ws.readyState !== WebSocket.OPEN) return;
          
          const inputData = e.inputBuffer.getChannelData(0);
          
          // Check audio level
          let sum = 0;
          let max = 0;
          for (let i = 0; i < inputData.length; i++) {
            sum += Math.abs(inputData[i]);
            max = Math.max(max, Math.abs(inputData[i]));
          }
          const avgLevel = sum / inputData.length;
          
          // Log audio level every 20 chunks (mÃ¡s frecuente para debug)
          if (audioChunksSent % 20 === 0) {
            console.log(`Audio level - avg: ${avgLevel.toFixed(4)}, max: ${max.toFixed(4)}`);
          }
          
          // Voice Activity Detection (VAD)
          const SPEECH_THRESHOLD = 0.02;  // Aumentado para evitar falsos positivos
          const SILENCE_THRESHOLD = 0.01;   // Aumentado tambiÃ©n
          
          if (max > SPEECH_THRESHOLD) {
            speechCounter++;
            silenceCounter = 0;
            
            // Start of speech detected
            if (!isSpeaking && speechCounter > 2) {
              isSpeaking = true;
              console.log('ðŸŽ¤ Speech detected - interrupting agent');
              
              // Send interrupt signal
              ws.send(JSON.stringify({
                type: 'interrupt'
              }));
              
              // Stop playing agent audio
              audioQueue = [];
              isPlaying = false;
              
              // Stop current audio if playing
              if (currentAudioSource) {
                try {
                  currentAudioSource.stop();
                  currentAudioSource = null;
                } catch (e) {
                  // Already stopped
                }
              }
            }
          } else if (max < SILENCE_THRESHOLD) {
            silenceCounter++;
            speechCounter = 0;
            
            // End of speech detected
            if (isSpeaking && silenceCounter > 10) {
              isSpeaking = false;
              console.log('ðŸ”‡ Silence detected');
            }
          }
          
          // Only send audio if we're speaking
          if (!isSpeaking) {
            return;
          }
          
          // Amplify audio by 2x if it's too quiet
          const amplificationFactor = max < 0.1 ? 2.0 : 1.0;
          
          // Resample if needed (most browsers use 48000Hz)
          let processedData = inputData;
          if (needsResampling && audioContext.sampleRate === 48000) {
            // Simple downsampling from 48000 to 16000 (factor of 3)
            const downsampleFactor = 3;
            const newLength = Math.floor(inputData.length / downsampleFactor);
            const downsampled = new Float32Array(newLength);
            
            for (let i = 0; i < newLength; i++) {
              downsampled[i] = inputData[i * downsampleFactor];
            }
            processedData = downsampled;
          }
          
          // Convert float32 to PCM16 with amplification
          const pcm16 = new Int16Array(processedData.length);
          for (let i = 0; i < processedData.length; i++) {
            const amplified = processedData[i] * amplificationFactor;
            const s = Math.max(-1, Math.min(1, amplified));
            pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
          }
          
          // Convert to base64
          const buffer = new ArrayBuffer(pcm16.length * 2);
          const view = new DataView(buffer);
          for (let i = 0; i < pcm16.length; i++) {
            view.setInt16(i * 2, pcm16[i], true); // little-endian
          }
          
          const bytes = new Uint8Array(buffer);
          let binary = '';
          for (let i = 0; i < bytes.byteLength; i++) {
            binary += String.fromCharCode(bytes[i]);
          }
          const base64 = btoa(binary);
          
          // Send audio chunk
          ws.send(JSON.stringify({
            type: 'audio',
            data: base64
          }));
          
          audioChunksSent++;
          if (audioChunksSent % 10 === 0) {
            console.log(`Sent PCM audio chunk #${audioChunksSent} (amplified: ${amplificationFactor}x, size: ${base64.length} chars)`);
            // Log primeros valores del PCM para debug
            console.log(`First PCM values: ${pcm16.slice(0, 10).join(', ')}`);
          }
        };
        
        // Connect audio nodes
        source.connect(processor);
        processor.connect(audioContext.destination);
        
        console.log('Audio capture started with Web Audio API');
        
        // Store references for cleanup
        window.audioSource = source;
        window.audioProcessor = processor;
        window.isRecording = isRecording;
        
      } catch (error) {
        console.error('Failed to access microphone:', error);
        callStatusEl.textContent = 'Error de micrÃ³fono';
      }
    }

    async function playNextAudio() {
      if (audioQueue.length === 0) {
        isPlaying = false;
        return;
      }

      isPlaying = true;
      const base64Audio = audioQueue.shift();

      try {
        // Ensure AudioContext is running
        if (audioContext.state === 'suspended') {
          console.log('Resuming AudioContext for playback...');
          await audioContext.resume();
        }
        
        // Decode base64 to array buffer
        const binaryString = atob(base64Audio);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }

        // ElevenLabs sends PCM 16000Hz audio
        // Create audio buffer manually since it's raw PCM
        const sampleRate = 16000;
        const numSamples = bytes.length / 2; // 16-bit samples
        const audioBuffer = audioContext.createBuffer(1, numSamples, sampleRate);
        const channelData = audioBuffer.getChannelData(0);
        
        // Convert PCM16 to float32
        const dataView = new DataView(bytes.buffer);
        for (let i = 0; i < numSamples; i++) {
          const sample = dataView.getInt16(i * 2, true); // little-endian
          channelData[i] = sample / 32768.0; // normalize to [-1, 1]
        }
        
        // Play the audio
        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioContext.destination);
        source.onended = () => {
          currentAudioSource = null;
          playNextAudio();
        };
        
        // Store reference to stop if needed
        currentAudioSource = source;
        source.start();
        console.log('Playing audio chunk, duration:', audioBuffer.duration, 'seconds');

      } catch (error) {
        console.error('Error playing audio:', error);
        playNextAudio();
      }
    }

    // Update current time
    setInterval(updateCurrentTime, 1000);
    updateCurrentTime();

    // Create a click handler to resume audio on first interaction
    async function handleUserInteraction() {
      if (audioContext && audioContext.state === 'suspended') {
        try {
          await audioContext.resume();
          console.log('AudioContext resumed after user interaction');
        } catch (e) {
          console.error('Failed to resume AudioContext:', e);
        }
      }
    }
    
    // Add click handlers to all buttons
    document.querySelectorAll('button').forEach(btn => {
      btn.addEventListener('click', handleUserInteraction);
    });
    
    // Also add to the whole document for any click
    document.addEventListener('click', handleUserInteraction, { once: true });
    
    // Initialize call when page loads
    initializeCall();

    // Button interactions
    document.getElementById('speakerBtn').addEventListener('click', function() {
      this.classList.toggle('active');
    });

    document.getElementById('muteBtn').addEventListener('click', function() {
      this.classList.toggle('active');
      if (window.isRecording !== undefined) {
        window.isRecording = !this.classList.contains('active');
        console.log('Recording:', window.isRecording ? 'enabled' : 'muted');
      }
    });

    document.getElementById('endBtn').addEventListener('click', function() {
      if (ws) {
        ws.send(JSON.stringify({ type: 'end' }));
        ws.close();
      }
      window.location.href = '/dialer';
    });
  </script>

</body>
</html>